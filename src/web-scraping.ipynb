{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "demographic-accuracy",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "In the following steps, we are doing the Web Scraping for fetching the details of the Organization.\n",
    "Step 1: Importing the imports\n",
    "\"\"\"\n",
    "\n",
    "import xlrd\n",
    "from selenium import webdriver\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import xlwt\n",
    "from xlwt import Workbook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "impressed-digit",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Step 2: creating object for Excel Sheet \"\"\"\n",
    "wb1 = Workbook()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "comprehensive-signal",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Step 3 : creating Sheet in Excel\"\"\"\n",
    "sheet1 = wb1.add_sheet('Sheet 1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "guided-walker",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Step 4: Variables for Columns and Rows to handle the loop\"\"\"\n",
    "row = 0\n",
    "col = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "hybrid-semiconductor",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Step 5: Variable for Path of Excel Sheet containing the dataset to used for Scraping\"\"\"\n",
    "loc = (\"web-scraping.xls\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "polished-dealer",
   "metadata": {},
   "source": [
    "Following the suggestion from https://askubuntu.com/questions/539498/where-does-chromedriver-install-to I was able to make it work like this:\n",
    "\n",
    "* Installed the chromium-chromedriver:\n",
    "```shell\n",
    "sudo apt-get install chromium-chromedriver\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "hearing-comparison",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Note: Google Chrome Version 88.0.4324.182 (Official Build) (64-bit)\n",
    "\n",
    "Step 6: Variable for setting the test web browser drivers location\n",
    "\"\"\"\n",
    "driver = webdriver.Chrome(\"/usr/lib/chromium-browser/chromedriver\")\n",
    "#driver = webdriver.Chrome(\"C:/Users/Thinksprout Infotech /Desktop/chromedriver_win32/chromedriver\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "macro-sandwich",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 7: Workbook object to read the Dataset from excel\n",
    "wb = xlrd.open_workbook(loc)\n",
    "sheet = wb.sheet_by_index(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "settled-south",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Thinksprout Infotech\n"
     ]
    }
   ],
   "source": [
    "# Step 8: traversing the Dataset\n",
    "# Continue the Loop till end\n",
    "for r in range(sheet.nrows):\n",
    "    name = sheet.cell_value(r, 0)  # url is in first column\n",
    "    \n",
    "    # Step 9: URL that is to be web scraped\n",
    "    #HiddenData is the URL of website to be webscrapped\n",
    "    #brackets ‘{ }’ represents the data for parameters to the website URL\n",
    "    stuff_in_string = \"https://{}/\".format(name)\n",
    "    \n",
    "    # Step 10: Sending data of URL to the Web browser\n",
    "    driver.get(stuff_in_string)\n",
    "    content = driver.page_source\n",
    "    soup = BeautifulSoup(content,\"html.parser\")\n",
    "    \n",
    "    # Step 11: Data for actual Website Tags that has to scraped and Data has to be fetched\n",
    "    content1 = soup.find('h1', {\"class\": \"elementor-heading-title elementor-size-default\"})\n",
    "    article = ''\n",
    "        \n",
    "    # Step 12: Exporting Data to New Excel-Sheet\n",
    "    for i in content1.findAll('strong'):\n",
    "        article = i.get_text()\n",
    "        print(article)\n",
    "        sheet1.write(row, col, name)\n",
    "        sheet1.write(row, col+1, article)\n",
    "        wb1.save('web-scraping-result.xls')\n",
    "        row+=1\n",
    "        \n",
    "    # Step 13: Closing the Test Chrome Browser\n",
    "    #outside the second for loop i.e inside second first for loop\n",
    "    driver.quit() \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "municipal-communist",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ws37",
   "language": "python",
   "name": "ws37"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
